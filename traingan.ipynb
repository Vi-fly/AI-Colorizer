{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1913658,"sourceType":"datasetVersion","datasetId":1036526}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# -----------------------------------\n# 1. U-Net Generator\n# -----------------------------------\nclass UNetGenerator(nn.Module):\n    def __init__(self, input_channels=1, output_channels=3):\n        super(UNetGenerator, self).__init__()\n\n        def down_block(in_channels, out_channels, normalize=True):\n            layers = [nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_channels))\n            layers.append(nn.LeakyReLU(0.2))\n            return nn.Sequential(*layers)\n\n        def up_block(in_channels, out_channels, dropout=False):\n            layers = [nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)]\n            layers.append(nn.BatchNorm2d(out_channels))\n            if dropout:\n                layers.append(nn.Dropout(0.5))\n            layers.append(nn.ReLU())\n            return nn.Sequential(*layers)\n\n        self.down1 = down_block(input_channels, 64, normalize=False)\n        self.down2 = down_block(64, 128)\n        self.down3 = down_block(128, 256)\n        self.down4 = down_block(256, 512)\n        self.down5 = down_block(512, 512)\n        self.down6 = down_block(512, 512)\n        self.down7 = down_block(512, 512)\n        self.down8 = down_block(512, 512, normalize=False)\n\n        self.up1 = up_block(512, 512, dropout=True)\n        self.up2 = up_block(1024, 512, dropout=True)\n        self.up3 = up_block(1024, 512, dropout=True)\n        self.up4 = up_block(1024, 512)\n        self.up5 = up_block(1024, 256)\n        self.up6 = up_block(512, 128)\n        self.up7 = up_block(256, 64)\n\n        self.final = nn.ConvTranspose2d(128, output_channels, kernel_size=4, stride=2, padding=1)\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down7(d6)\n        d8 = self.down8(d7)\n\n        u1 = self.up1(d8)\n        u1 = torch.cat([u1, d7], dim=1)\n        u2 = self.up2(u1)\n        u2 = torch.cat([u2, d6], dim=1)\n        u3 = self.up3(u2)\n        u3 = torch.cat([u3, d5], dim=1)\n        u4 = self.up4(u3)\n        u4 = torch.cat([u4, d4], dim=1)\n        u5 = self.up5(u4)\n        u5 = torch.cat([u5, d3], dim=1)\n        u6 = self.up6(u5)\n        u6 = torch.cat([u6, d2], dim=1)\n        u7 = self.up7(u6)\n        u7 = torch.cat([u7, d1], dim=1)\n\n        final = self.final(u7)\n        return self.tanh(final)\n\n\n# -----------------------------------\n# 2. PatchGAN Discriminator\n# -----------------------------------\nclass PatchGANDiscriminator(nn.Module):\n    def __init__(self, input_channels=4):  # (Gray + Color Image)\n        super(PatchGANDiscriminator, self).__init__()\n\n        def discriminator_block(in_channels, out_channels, normalization=True):\n            layers = [nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)]\n            if normalization:\n                layers.append(nn.BatchNorm2d(out_channels))\n            layers.append(nn.LeakyReLU(0.2))\n            return nn.Sequential(*layers)\n\n        self.model = nn.Sequential(\n            discriminator_block(input_channels, 64, normalization=False),\n            discriminator_block(64, 128),\n            discriminator_block(128, 256),\n            discriminator_block(256, 512),\n            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, gray_img, color_img):\n        combined = torch.cat([gray_img, color_img], dim=1)\n        return self.model(combined)\n\n\n# # -----------------------------------\n# # Testing the Models (Optional)\n# # -----------------------------------\n# if __name__ == \"__main__\":\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n#     # Test Generator\n#     generator = UNetGenerator().to(device)\n#     input_tensor = torch.randn(1, 1, 256, 256).to(device)  # 1 grayscale image\n#     output_tensor = generator(input_tensor)\n#     print(f\"Generator Output Shape: {output_tensor.shape}\")  # Should be (1, 3, 256, 256)\n\n#     # Test Discriminator\n#     discriminator = PatchGANDiscriminator().to(device)\n#     gray_tensor = torch.randn(1, 1, 256, 256).to(device)  # 1 grayscale image\n#     color_tensor = torch.randn(1, 3, 256, 256).to(device)  # 1 color image\n#     output_disc = discriminator(gray_tensor, color_tensor)\n#     print(f\"Discriminator Output Shape: {output_disc.shape}\")  # Should be (1, 1, 30, 30)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:06:06.980468Z","iopub.execute_input":"2025-02-07T18:06:06.980692Z","iopub.status.idle":"2025-02-07T18:06:09.920027Z","shell.execute_reply.started":"2025-02-07T18:06:06.980671Z","shell.execute_reply":"2025-02-07T18:06:09.919110Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom tqdm import tqdm\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Hyperparameters\nBATCH_SIZE = 8\nIMAGE_SIZE = 256\nEPOCHS = 100\nLEARNING_RATE = 0.0002\nBETA1 = 0.5\n\n# Dataset Class\nclass ColorizationDataset(Dataset):\n    def __init__(self, gray_dir, color_dir, transform=None):\n        self.gray_dir = gray_dir\n        self.color_dir = color_dir\n        self.transform = transform\n        self.image_filenames = os.listdir(gray_dir)\n\n    def __len__(self):\n        return len(self.image_filenames)\n\n    def __getitem__(self, idx):\n        gray_path = os.path.join(self.gray_dir, self.image_filenames[idx])\n        color_path = os.path.join(self.color_dir, self.image_filenames[idx])\n\n        gray_image = Image.open(gray_path).convert(\"L\")  # Load as grayscale\n        color_image = Image.open(color_path).convert(\"RGB\")  # Load as RGB\n\n        if self.transform:\n            gray_image = self.transform(gray_image)\n            color_image = self.transform(color_image)\n\n        return gray_image, color_image\n\n# Data Transformations\ntransform = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n\n# Load Dataset\ndataset = ColorizationDataset(\"/kaggle/input/landscape-image-colorization/landscape Images/gray\", \"/kaggle/input/landscape-image-colorization/landscape Images/color\", transform=transform)\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n# Initialize Models\ngenerator = UNetGenerator().to(device)\ndiscriminator = PatchGANDiscriminator().to(device)\n\n# Loss Functions\ncriterion_GAN = nn.BCEWithLogitsLoss()\ncriterion_L1 = nn.L1Loss()  # L1 loss for better realism\n\n# Optimizers\noptimizer_G = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(BETA1, 0.999))\noptimizer_D = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(BETA1, 0.999))\n\n# Training Loop\nfor epoch in range(EPOCHS):\n    for gray_images, real_images in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n\n        gray_images, real_images = gray_images.to(device), real_images.to(device)\n\n        # 1. Train Discriminator\n        optimizer_D.zero_grad()\n\n        # Real images\n        real_labels = torch.ones((gray_images.size(0), 1, 15, 15), device=device)  # PatchGAN output size (adjusted)\n        fake_labels = torch.zeros((gray_images.size(0), 1, 15, 15), device=device)\n\n        output_real = discriminator(gray_images, real_images)\n        loss_real = criterion_GAN(output_real, real_labels)\n\n        # Fake images\n        fake_images = generator(gray_images)\n        output_fake = discriminator(gray_images, fake_images.detach())\n        loss_fake = criterion_GAN(output_fake, fake_labels)\n\n        # Compute total loss and update discriminator\n        loss_D = (loss_real + loss_fake) / 2\n        loss_D.backward()\n        optimizer_D.step()\n\n        # 2. Train Generator\n        optimizer_G.zero_grad()\n\n        output_fake = discriminator(gray_images, fake_images)\n        loss_GAN = criterion_GAN(output_fake, real_labels)\n        loss_L1 = criterion_L1(fake_images, real_images) * 100  # L1 loss for realism\n\n        loss_G = loss_GAN + loss_L1\n        loss_G.backward()\n        optimizer_G.step()\n\n    # Save checkpoint every 10 epochs\n    if (epoch + 1) % 10 == 0:\n        torch.save(generator.state_dict(), f\"colorization_gan_{epoch+1}.pth\")\n\n# Save Final Model\ntorch.save(generator.state_dict(), \"colorization_gan.pth\")\nprint(\"Training Complete! Model Saved as colorization_gan.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:10:10.225990Z","iopub.execute_input":"2025-02-07T18:10:10.226281Z","iopub.status.idle":"2025-02-07T23:03:51.269751Z","shell.execute_reply.started":"2025-02-07T18:10:10.226262Z","shell.execute_reply":"2025-02-07T23:03:51.265712Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/100: 100%|██████████| 892/892 [03:23<00:00,  4.38it/s]\nEpoch 2/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 3/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 4/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 5/100: 100%|██████████| 892/892 [03:26<00:00,  4.33it/s]\nEpoch 6/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 7/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 8/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 9/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 10/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 11/100: 100%|██████████| 892/892 [03:26<00:00,  4.33it/s]\nEpoch 12/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 13/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 14/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 15/100: 100%|██████████| 892/892 [03:26<00:00,  4.33it/s]\nEpoch 17/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 18/100: 100%|██████████| 892/892 [03:26<00:00,  4.33it/s]\nEpoch 19/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 20/100: 100%|██████████| 892/892 [03:26<00:00,  4.33it/s]\nEpoch 21/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 23/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 24/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 25/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 26/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 28/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 29/100: 100%|██████████| 892/892 [03:26<00:00,  4.33it/s]\nEpoch 30/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 32/100: 100%|██████████| 892/892 [03:26<00:00,  4.31it/s]\nEpoch 33/100: 100%|██████████| 892/892 [03:26<00:00,  4.31it/s]\nEpoch 36/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 37/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 38/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 40/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 41/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 42/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 43/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 45/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 46/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 50/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 51/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 52/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 53/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 54/100: 100%|██████████| 892/892 [03:26<00:00,  4.33it/s]\nEpoch 55/100: 100%|██████████| 892/892 [03:26<00:00,  4.33it/s]\nEpoch 56/100: 100%|██████████| 892/892 [03:26<00:00,  4.33it/s]\nEpoch 58/100: 100%|██████████| 892/892 [03:26<00:00,  4.33it/s]\nEpoch 59/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 61/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 62/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 63/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 64/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 65/100: 100%|██████████| 892/892 [03:26<00:00,  4.33it/s]\nEpoch 66/100: 100%|██████████| 892/892 [03:26<00:00,  4.33it/s]\nEpoch 67/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 68/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 69/100: 100%|██████████| 892/892 [03:25<00:00,  4.33it/s]\nEpoch 70/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 71/100: 100%|██████████| 892/892 [03:26<00:00,  4.33it/s]\nEpoch 72/100: 100%|██████████| 892/892 [03:29<00:00,  4.25it/s]\nEpoch 73/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 74/100: 100%|██████████| 892/892 [03:26<00:00,  4.33it/s]\nEpoch 75/100: 100%|██████████| 892/892 [03:25<00:00,  4.33it/s]\nEpoch 76/100: 100%|██████████| 892/892 [03:25<00:00,  4.34it/s]\nEpoch 77/100: 100%|██████████| 892/892 [03:25<00:00,  4.33it/s]\nEpoch 78/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 79/100: 100%|██████████| 892/892 [03:26<00:00,  4.32it/s]\nEpoch 81/100: 100%|██████████| 892/892 [03:25<00:00,  4.33it/s]\nEpoch 82/100: 100%|██████████| 892/892 [03:26<00:00,  4.33it/s]\nEpoch 83/100: 100%|██████████| 892/892 [03:26<00:00,  4.33it/s]\nEpoch 84/100: 100%|██████████| 892/892 [03:26<00:00,  4.33it/s]\nEpoch 85/100: 100%|██████████| 892/892 [03:25<00:00,  4.34it/s]\nEpoch 86/100:  38%|███▊      | 337/892 [01:17<02:08,  4.32it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-ea62c96f3c5d>\u001b[0m in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgray_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_images\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{EPOCHS}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mgray_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgray_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# 1. Train Discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}